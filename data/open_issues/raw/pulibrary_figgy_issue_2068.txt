Generate HathiTrust ingestibles
See HathiTrust Ingest Checklist for requirements and specifications. There are three parts:  Administrative Coversheet Bibliographic Metadata Content  See also:  Guidelines for Digital Object Deposit SUBMITTING METADATA TO ZEPHIR HathiTrust Cloud Validation and Packaging Service Ingest Checklist Hathi's SIP validator  Can probably model this feature on the BagIt feature. Should be implemented using Figgy's background workers. There should probably be a checkbox in admin interface for objects: should this be sent to Hathi? Questions to answer:  Should this (also) be a Rake task? Should we try using Google's new cloud tool to generate hocr?
A meta.yml file is required. [Hathi provides a template to be used as a model](https://drive.google.com/file/d/0B0EHs5JWGUMLWjU2OHVhQzN5WEk/view) The following are required fields:  - capture_date - scanner_make - scanner_model - scanner_user - scanning_order - reading_order  The following are required if the data cannot be extracted from the TIFFs:  - bitonal_resolution_dpi (if bitonal TIFF) - contone_resolution_dpi (if contone TIFFs or JPEG2000 images) - image_compression_date - image_compression_agent - image_compression_tool  A **pagedata** field containing page numbers and page tags (labels) is optional.
Some of that seems pretty easy to get from the TIFF: * capture_date * scanner_make * scanner_model * bitonal/contone resolution_dpi  Some of that is typically not applicable for us: image_compression_date/agent/tool  reading_order we can get from the `viewing_direction` property  These seem harder to me: * scanning_order: I guess we could infer this from the TIFF timestamps? * scanner_user: This would typically be the `depositor` user for stuff scanned by the studio, but for vendor-digitized items, I don't think we know this. 
Since we don't compress the images, that metadata shouldn't be relevant, I'd think.
Branch for ongoing work is here: https://github.com/pulibrary/figgy/tree/issue-2068