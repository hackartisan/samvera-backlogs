Minimize spider traffic
Now that we're in the cloud and there are costs, we need to pay more attention to network transit We still want to make sure traffic is not disruptive to the application. Instead of taking immediate action, Infrastructure should take a look at the available features and hints for spiders and verify they will minimize abuse from well-behaved spiders. One thing that could cost us a lot is if we aren't providing spiders hints about what content hasn't changed. We don't want them constantly pulling unchanging content. This is especially true when it involves images or other bitstreams. Some possible controls we have include:  setting lastmod and changefreq fields in sitemaps serving up appropriate last-modified response headers in the replies properly handling if-modified-since headers in requests  I'm sure there are others. It's also quite possible that different spiders (google, bing, baidu, etc.) are using slightly different methods to determine when to redownload content.